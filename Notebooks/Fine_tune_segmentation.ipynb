{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dependencies and install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripts directory: i:\\My Drive\\Flow_segmentation\\Scripts\\libs\n",
      "Pulling the latest version of lightning-sam...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "repos = {\n",
    "    \"lightning-sam\": \"https://github.com/luca-medeiros/lightning-sam.git\"\n",
    "}\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "scripts_dir = os.path.join(parent_dir, 'Scripts', 'libs')\n",
    "\n",
    "sys.path.append(os.path.dirname(scripts_dir))\n",
    "print(f\"Scripts directory: {scripts_dir}\")\n",
    "\n",
    "os.makedirs(scripts_dir, exist_ok=True)\n",
    "\n",
    "for repo_name, repo_url in repos.items():\n",
    "    repo_path = os.path.join(scripts_dir, repo_name)\n",
    "\n",
    "    if os.path.isdir(repo_path):\n",
    "        print(f\"Pulling the latest version of {repo_name}...\")\n",
    "        subprocess.run(['git', 'pull'], cwd=repo_path, check=True)\n",
    "    else:\n",
    "        print(f\"Cloning the repository {repo_name}...\")\n",
    "        subprocess.run(['git', 'clone', repo_url, repo_path], check=True)\n",
    "\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        for dir in dirs:\n",
    "            full_path = os.path.join(root, dir)\n",
    "            if full_path not in sys.path:\n",
    "                sys.path.append(full_path)\n",
    "\n",
    "\n",
    "    # Install the dependencies\n",
    "    requirements_path = os.path.join(repo_path, 'requirements.txt')\n",
    "    if os.path.isfile(requirements_path):\n",
    "        print(f\"Installing dependencies for {repo_name}...\")\n",
    "        subprocess.run(['pip', 'install', '-r', requirements_path], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               Value\n",
      "num_devices                                                                                        0\n",
      "batch_size                                                                                         1\n",
      "num_workers                                                                                        2\n",
      "num_epochs                                                                                         1\n",
      "eval_interval                                                                                      2\n",
      "out_dir                                         i:\\My Drive\\Flow_segmentation\\Checkpoints_models\\out\n",
      "opt_learning_rate                                                                             0.0008\n",
      "opt_weight_decay                                                                              0.0001\n",
      "opt_decay_factor                                                                                  10\n",
      "opt_steps                                                                             [60000, 86666]\n",
      "opt_warmup_steps                                                                                 250\n",
      "model_type                                                                                     vit_h\n",
      "model_checkpoint               i:\\My Drive\\Flow_segmentation\\Checkpoints_models\\sam_vit_h_4b8939.pth\n",
      "model_freeze_image_encoder                                                                      True\n",
      "model_freeze_prompt_encoder                                                                     True\n",
      "model_freeze_mask_decoder                                                                      False\n",
      "dataset_train_root_dir                                 i:\\My Drive\\Flow_segmentation\\Demo\\Train_data\n",
      "dataset_train_annotation_file    i:\\My Drive\\Flow_segmentation\\Demo\\Train_data\u0007nnotations_train.json\n",
      "dataset_val_root_dir                                   i:\\My Drive\\Flow_segmentation\\Demo\\Train_data\n",
      "dataset_val_annotation_file        i:\\My Drive\\Flow_segmentation\\Demo\\Train_data\u0007nnotations_val.json\n"
     ]
    }
   ],
   "source": [
    "from Fine_tune_config import ft_cfg\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "flat_config = pd.json_normalize(ft_cfg, sep='_')\n",
    "config_df = flat_config.transpose()\n",
    "config_df.columns = ['Value']\n",
    "print(config_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "from model import Model\n",
    "from dataset import load_datasets\n",
    "import torch.nn.functional as F\n",
    "from train import train_sam, validate, configure_opt\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "# fabric = L.Fabric(accelerator=\"auto\",\n",
    "#                   devices=ft_cfg.num_devices,\n",
    "#                   strategy=\"auto\",\n",
    "#                   loggers=[TensorBoardLogger(ft_cfg.out_dir, name=\"lightning-sam\")])\n",
    "\n",
    "accelerator = \"cpu\" if ft_cfg.num_devices == 0 else \"auto\"\n",
    "\n",
    "fabric = L.Fabric(\n",
    "    accelerator=accelerator,\n",
    "    devices=ft_cfg.num_devices,\n",
    "    strategy=\"auto\",\n",
    "    loggers=[TensorBoardLogger(ft_cfg.out_dir, name=\"lightning-sam\")]\n",
    ")\n",
    "\n",
    "fabric.launch()\n",
    "fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    os.makedirs(ft_cfg.out_dir, exist_ok=True)\n",
    "\n",
    "with fabric.device:\n",
    "    model = Model(ft_cfg)\n",
    "    model.setup()\n",
    "\n",
    "train_data, val_data = load_datasets(ft_cfg, model.model.image_encoder.img_size)\n",
    "train_data = fabric._setup_dataloader(train_data)\n",
    "val_data = fabric._setup_dataloader(val_data)\n",
    "\n",
    "optimizer, scheduler = configure_opt(ft_cfg, model)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "train_sam(ft_cfg, fabric, model, optimizer, scheduler, train_data, val_data)\n",
    "validate(fabric, model, val_data, epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from box import Box\n",
    "from dataset import COCODataset\n",
    "from model import Model\n",
    "from tqdm import tqdm\n",
    "from utils import draw_image\n",
    "\n",
    "def visualize(cfg: Box):\n",
    "    model = Model(cfg)\n",
    "    model.setup()\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    dataset = COCODataset(root_dir=cfg.dataset.val.root_dir,\n",
    "                          annotation_file=cfg.dataset.val.annotation_file,\n",
    "                          transform=None)\n",
    "    \n",
    "    predictor = model.get_predictor()\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    for image_id in tqdm(dataset.image_ids):\n",
    "        image_info = dataset.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(dataset.root_dir, image_info['file_name'])\n",
    "        image_output_path = os.path.join(cfg.out_dir, image_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ann_ids = dataset.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = dataset.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "        bboxes = torch.as_tensor(bboxes, device=model.model.device)\n",
    "        transformed_boxes = predictor.transform.apply_boxes_torch(bboxes, image.shape[:2])\n",
    "        predictor.set_image(image)\n",
    "        masks, _, _ = predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        image_output = draw_image(image, masks.squeeze(1), boxes=None, labels=None)\n",
    "        cv2.imwrite(image_output_path, image_output)\n",
    "\n",
    "\n",
    "visualize(ft_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opendrift_env)",
   "language": "python",
   "name": "opendrift_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
